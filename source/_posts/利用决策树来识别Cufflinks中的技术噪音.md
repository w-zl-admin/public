---
title: 利用决策树来识别Cufflinks中的技术噪音
date: 2015-07-26 15:38:48
categories:
- 机器学习
tags:
- RNA-seq
- 决策树
- Cufflinks
- 技术噪音
---

# 简介
熟悉RNA-Seq分析流程的朋友大多都知道，在经典的TopHat-Cufflinks分析流程中，对于最终的转录子（transfrag）组装的结果，很难去判断是否真实存在。在实际操作中，我们一般采用简单的一刀切，来进行过滤。比如，对于FPKM<1的进行过滤。这种方法虽然很有效，但是可能会丢失部分有价值的信息。在这篇文章中，我向大家介绍一种基于决策树的机器学习方法来识别Cufflins结果中的技术噪音（Tech Noise）。
## 决策树
### 什么是决策树
决策树是一种依托选择策略建立的图解法。在机器学习中，决策树属于有督导学习（Supervised Learning）中分类（Classification）算法范围。决策树，经常出现在我们生活中。比如，

>顾客：这手机能拍照吗？  
>销售：能！  
>顾客：这手机能上网吗？
>销售：能！ 
>顾客：这手机能防水吗？  
>销售：能！ 
>顾客：我买了！ 

用图解法表示决策过程：
![买手机决策过程图解](/upload/1/decisionTreeSample.png '买手机决策树')
这样，有了上面的认识，我们可以发现：
- 决策树是一个树结构（可以是二叉树或非二叉树）。
- 决策树上的每个非叶节点表示了对某一属性的判断。
- 非叶节点的分支表示该属性的判断结果。
- 叶节点表示分类结果。

### 决策树分类算法
决策树中最关键步骤是分裂属性。什么是分裂属性？分裂属性是指在某个节点，根据对某个属性的判断来把该节点集合分裂为两个或多个子集合（二叉树或多叉树）。理想的分裂，应该使各个子集合中的各个元素的类别一致。这样一来，分裂属性的情况包括以下三种：
- 属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。
- 属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。
- 属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。  

这样看来，属性的量度对于决策树的构造非常重要。选择属性度量的算法有很多，大部分都是采用自顶向下递归分治法（Recursive Partitioning），同时采用不回溯的贪心策略（Greedy Strategy）。
#### ID3算法
ID3算法，全称Iterative Dichotomiser 3 ，译名迭代二叉树3代，1986年由Quinlan提出。ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。即在节点处选择当前集合中最大信息增益的属性作为分裂属性，并在子集中不断递归。以下是信息论中的基本概念：
- 信息熵（Entropy）
若信源符号有n种取值：$U_1$…$U_i$…$U_n$，对应概率为：$P_1$…$P_i$…$P_n$，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性$-logP_i$的统计平均值（$E$），可称为信息熵，即$$H(U)=E\left | -logP\_i \right |=-\sum_{i=0}^{n}P\_ilogP\_i$$式中对数一般取2为底，单位为比特。但是，也可以取其它对数底，采用其它相应的单位，它们间可用换底公式换算。 信息熵是表示随机变量不确定性的度量。从直观上，信息熵越大，变量包含的信息量越大，变量的不确定性也越大。一个事物内部会存在随机性，也就是不确定性，而从外部消除这个不确定性唯一的办法是引入信息。如果没有信息，任何公式或者数字的游戏都无法排除不确定性。
![信息熵函数](/upload/1/EntropyFunction.png '信息熵函数')
![二元信息熵](/upload/1/towbit.png '二元信息熵')
- 复合熵（Composite Entropy）
根据贝叶斯定理，结合信息熵。复合熵函数：$$H(x,y)=E|-logP\_{(x,y)}|=\sum\_{x}^{ }\sum\_{y}^{ }P\_{(x,y)}logP\_{(x,y)}=-\sum\_{x}^{ }\sum\_{y}^{ }P\_xP\_ylog(P\_xP\_y)$$
- 条件熵（conditional Entropy）
根据贝叶斯定理，结合信息熵。条件熵函数：$$H(X|Y)=E|E|-logP\_{(X|Y)}||=-\sum\_{Y}^{}\sum\_{X}^{}P\_{Y\_j}P\_{(X\_i|Y\_j)}logP\_{(X\_i|Y\_j)}$$
- 信息增益（IG，Information Gain）
信息增益在信息论中是很有效的特征选择方法。但凡是特征选择，总是将特征的重要程度先进行量化后再进行选择，而如何量化特征的重要性，就成了各种方法最大的不同。在信息增益中，量化特征的重要性就是看特征能够为分类系统带来多少的信息，带来的信息越多，该特征就越为重要。这可以用信息熵去量化。另外需要注意的是，信息增益是针对特征而言，也就是系统中有没有某个特征间系统信息熵的变化。令$A$为分裂特征，$A\_i$为A特征分裂子集，$S\_n$为n类样本，$U$为所有样本集合，信息增益函数可以表示为：$$IG(A)=H(U)-H(U|A)=\sum\_{j=1}^{S}\sum\_{i=1}^{A}P\_{A\_i}P\_{S\_j|A\_i}log(P\_{S\_j|A\_i})-\sum\_{j=1}^{S}P\_{(S\_j)}log(P\_{(S\_j)})$$


ID3算法就是利用信息增益来量化特征的重要性。其理论在于，每次分裂实在使系统信息增益最大化的特征上进行。同时ID3只有正负两个样本（全局二叉树），根据信息增益函数，ID3可以表示为：
$$\max\_{i}(IG(A\_i))=\max\_{i}(H(U)-H(U|A))=$$

$$\max\_{i}(P\_A(P\_{(\oplus|A)}logP\_{(\oplus|A)}+P\_{(\ominus|A)}logP\_{(\ominus|A)})+(1-P\_A)(P\_{(\oplus|C\_SA)}logP\_{(\oplus|C\_SA)}+P\_{(\ominus|C\_SA)}logP\_{(\ominus|C\_SA)})-P\_{\oplus}logP\_{\oplus}-P\_{\ominus}logP\_{\ominus})$$
##### 实例
有类数据如下表，m,n为两个特征，c表示分类。

| m | n | c |
|----|----|----|
| + | + | + |
| + | + | + |
| + | - | + |
| - | + | - |
| - | - | - |
| + | - | - |

根据ID3算法对其建决策树：
1. 以m特征进行分裂的信息增量。
$$IG(m)=H(U)-H(U|A)=-(\frac{1}{2}log\_2\frac{1}{2}+\frac{1}{2}log\_2\frac{1}{2})-(-\frac{4}{6}(\frac{1}{4}log\_2\frac{1}{4}+\frac{3}{4}log\_2\frac{3}{4})-\frac{2}{6}(0+0))=0.459$$
![以m特征分裂](/upload/1/mclass.png '以m特征分裂')
2. 以n特征进行分裂的信息增量。
$$IG(n)=H(U)-H(U|A)=-(\frac{1}{2}log\_2\frac{1}{2}+\frac{1}{2}log\_2\frac{1}{2})-(-\frac{3}{6}(\frac{1}{3}log\_2\frac{1}{3}+\frac{2}{3}log\_2\frac{2}{3})-\frac{3}{6}(\frac{1}{3}log\_2\frac{1}{3}+\frac{2}{3}log\_2\frac{2}{3}))=0.082$$
![以n特征分裂](/upload/1/nclass.png '以n特征分裂')
3. 因为$IG(m)>IG(n)$，选择信息增益最大的特征进行分裂，所以选择m进行分裂。


#### C4.5算法
C4.5算法是ID3算法进行改进算法，主要不同点是对特征重要性量化上采用的是信息增益率（Info Gain Ratio）。这种方法解决ID3算法在某个属性存在大量不同值时导致的过拟合现象。相比于ID3算法，改进有如下几个要点：
- 用信息增益率来选择属性。
- 在决策树构造过程中可以进行剪枝。ID3算法会由于某些具有很少元素的结点可能会使构造的决策树过适应（Overfitting），这时候不考虑这些结点可能会更好。
- 对非离散数据也能处理。
- 能够对不完整数据进行处理。

##### 信息增益率
分裂属性$A$，分裂后子集空间$S$，信息增益率IGR则可表示为：
$$IGR(S,A)=\frac{IG(S,A)}{II(S,A)}$$
其中II表示内在信息（Intrinsic Information），其定义为：
$$II(S,A)=-\sum\frac{|S\_i|}{|S|}log\_2\frac{|S\_i|}{|S|}$$
其中$S\_i$是属性A分割S而形成的$i$子集。

##### 实例
依旧上述例子
- 以m特征进行分裂的信息增量。$$IGR(m)=\frac{IG(m)}{II{m}}=\frac{0.459}{\frac{4}{6}log\_2\frac{4}{6}+\frac{2}{6}log\_2\frac{2}{6}}=\frac{0.459}{0.918}=0.5$$

#### CART算法
分类和回归树（Classification and regression tree，CART）在模式分类中被视作一种通用的树生长算法。CART提供一种通用框架，利用它，可以实例化各种各样不同的判定树。按照CART，有6个问题需要回答：
- 节点分支数应该是多少？
- 节点测试是哪个属性？
- 何时可以令某节点成为叶节点？
- 如何剪枝？
- 如果叶节点仍不“纯”，那么怎样给它赋类别标记？
- 缺省数据处理？

##### 分支数目
任意多类别样本都可以通过多个二叉树进行划分。所以，二叉树具有万能的表达能力，并且在训练上很简便。所以，CART一般采用二叉树结构。
##### 节点不纯度
CART中有四种方式来量化不纯度（Impurity）：
- 信息熵不纯度（Entropy Impurity）$$I(N)=-sum\_{j}^{}P(w\_j)log\_2P(w\_j)$$
- 方差不纯度(Variance Impurity) $$I(N)=\prod\_{j}P(w\_j)=P(1-P)$$
- Gini不纯度(Gini Impurity)$$I(N)=\sum\_{j}^{}P(w\_j)P(w\_{C\_{S}j})=1-\sum\_{j}^{}P^2(w\_j)$$
- 误分类不纯度(Missclassification Impurity)$$I(N)=1-\max\_{j}P(w\_j)$$
![4种不纯度量化方法](/upload/1/class.png '4种不纯度量化方法')

在如何选择分裂属性上，很明显的启发思路是选择使不纯度下降最快的那个属性（ID3算法）。可记作$$\max\_{i}{\Delta}I(N)=I(N)-P\_LI(N\_L)-(1-P\_L)I(N\_R)$$，其中$N\_L$和$N\_R$分别是左右子节点。这种贪心算法有个缺点，就是无法确保顺序局部优化过程能得到全局最优。实践中，熵不纯度和Gini不纯度运用广泛，但在实践过程中，不同的不纯度函数对最后的分类效果和其性能影响往往比预期的小。所以，在实践中，反倒是停止判决和剪枝算法受到更多的重视。
##### 分支停止准则
二叉树，分支训练中，分支过多会过拟合，分支过少，训练样本误差大，分类效果也差。究竟何时应该停止分支呢？这里介绍五种方法：
1. 验证和交叉验证技术（validation and cross-validation）。
将部分样本用于训练树，剩余样本作为验证。持续节点分支，直到对于验证集的分类误差最小化。
2. 不纯度下降门限。
预先设定一个不纯度下降差的（小）门限值，当候选分支小于这个值时，停止分支。这种方法有两个优点，一是全部样本均用于训练，二是树的每层都可能存在叶节点。但该方法也有个固有缺点，即门限值预先设定相当困难，因为最终性能与门限大小并无直接的函数关系
3. 节点最小样本数。
当候选分支样本数小于设定值，停止分支。这种方法类似k-近临分类器，当样本集中时，分割子集就小；当样本稀疏时，分割子集就大。
4. 高复杂度换取高的准确度。
通过最小化全局指标：$$\alpha{\cdot}size+\sum\_{叶节点}^{}I(N)$$这里$size$表示节点或分支的数目，$\alpha$是个正常数（类似神经网络中的正则化方法）。这种方法中，$\alpha$的设定也很难，它也与最终分类性能无简单的相关关系。
5. 不纯度下降的显著性检验。
估计之前的所有${\Delta}I(N)$概率分布，在对某一候选分支$i$时，检验${\Delta}I(N\_i)$是否与上述分布存在统计学差异。
![卡方检验](/upload/1/chisq.png '采取卡方检验。对于候选分支，其理论发生概率$p$对应得卡方值函数为$\frac{(1-p)^2}{p}$，在0.05的置信水平下，$p$应该大于0.18。')
6. $\chi^2$假设检验。
要点是判断候选分支是否存在统计学上的意义，也就是判断分支是否明显有别于随机分支。假设节点存在$n$个样本（$n\_1$个$w\_1$类，$n\_2$个$w\_2$类），候选分支将$Pn$个样本分到左分支，$(1-P)n$分到了右支。此时假设是随机分支，左枝应该有$Pn\_1$个$w\_1$和$Pn\_2$个$w\_2$，其它剩余的都在右支。如果用$\chi^2$来评估这次分支与随机的偏离度，那偏离度$$\chi^2=\sum\_{i=1}^{2}\frac{(n\_{iL}-n\_{ie})^2}{n\_{ie}}$$其中$n\_{iL}$是指$w\_i$在左分支的数目，而$n\_{ie}=Pn\_i$则对应随机分支情况下的值。当两者相同时，$\chi^2$接近0，相反，当$\chi^2$越大，说明两者差异越大。当其大于某临界值时，就可以拒绝零假设（候选分支等于随机分支）。
7. 损失矩阵
构建损失矩阵，根据损失矩阵计算节点损失，当叶节点的损失大于其父节点的损失时，则停止分支。

##### 剪枝
有时，分支停止方法会因缺少足够的前瞻性而过早停止（视界局限效应）。这时，需要剪枝（pruning）来克服这种缺陷。在剪枝过程中，树首先要充足生长，直到叶节点都有最小不纯度值为止，因而没有任何推定的“视界局限”。然后，对所有相邻的成对叶节点，考虑能否合并它们，如果合并它们只引起很小的不纯度增长，那就合并它们为新的叶节点。

## cufflinks中决策树的理论基础
cufflinks软件原理是将mapped reads组装成transfrag，由于reads很短，拼接还原难度高。这个过程中可能会产生错误的结果。我们假设：
1. 同一染色体上的基因的特征相似。
2. cufflinks产生的未知的transfrag大部分是noise，可靠的transfrag仅占小部分，这些可靠的transfrag拥有与正常基因相似的特征，且与noise存在较大差异。
这样，我们就拥有了一个正样本集（真实基因），和一个负样本集（noise），这就可以建立决策树，来确认真实基因的特征。
在算法上，采取常用的CART算法，软件选用R语言及其rpart模块。

## 特征选择
该次选取了个5特征变量：长度，外显子数目，样本中表达比例，FPKM值。

|特征变量|选择原因|
|--------|----------------|
|  长度  | 人类基因90%以上都在300bp以上，过短的transfrag可能是由于技术噪音产生 |
| 外显子数 | 人类大部分基因都拥有多个外显子数，趋于更为复杂的结构；而技术噪音一般都很简单。|
| 样本中表达比例 | 正常的基因，应该会在多个样本中都会出现表达，而部分技术噪音可能仅在个别样本中表达。 |
| FPKM值 | 正常基因的FPKM值应该会在一定范围内，而技术噪音序列可能会存在FPKM极值。 |


# 数据建模
## 数据描述
数据使用的是10个人类样本cufflinks组装结果中1号染色体上的数据。
数据集中共28,290个转录子，其中26,185是已知转录位点，2,105是未知转录位点。

|特征变量|最小值|中值|均值|最大值|
|------|-----|-----|-----|-------|
| 长度 | 1 | 1540 | 2679 | 79680 |
| 外显子数 | 1 | 5 | 7.887 | 100 |
| 表达比例 | 0.10 | 0.50 | 0.54 | 1.00 |
| FPKM | 0 | 0.115 | 2.131 | 5271 |


![数据描述](/upload/1/summary.png '数据描述(已知转录和未知转录并不能明显区分开)')
![PCA分析](/upload/1/pca_before.png 'PCA结果（无法区分）')
## rpart包参数简介
```
函数：rpart
用法：rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)
参数：
	formula：公式，例Y~X1+X2+X3+X4
	data：数据源，一般是数据框
	weights: 向量，各个自变量的权重
	subset：只用特定列
	method：如果Y是生存类，选择"exp"；如果Y有两列，则选择"poisson"；如果Y是因子，则选择"class"；如果Y是连续变量,则选择"anova";
	parms：分裂参数；"anova"类没有该参数，"exp"和"poisson"仅有先验分布；"class"有三项，prior对先验概率，loss对损失矩阵，split对不纯度计算方式———"gini"或"information"。
	control：参见rpart.control
	cost：各自变量代价列表
函数：rpart.control
用法：rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30, ...)
参数：
	minsplit：父节点最小样本数。
	minbucket：叶节点最小样本数。
	cp：complexity parameter.复杂性参数。分支后cp必须高于设定值。
	maxcompete：保留的最大候选分支数。
	maxsurrogate：替代分裂最大数。
	usesurrogate：替代分裂处理。0，仅展示；1，使用替代分裂；2，不使用替代分裂
	xval：cv数
	surrogatestyle：替代分裂风格。0，数值；1，比例。
	maxdepth：树的最大深度
```
## 代码
```R
# 加载rpart包
library(rpart)
# 构建数据框
d <- data.frame(length,exon,ratio,FPKM,class)
# 训练集和测试集
train.l <- sample(1:dim(d)[1],as.integer(dim(d)[1]*0.7)) #取样列
train <- d[train.l,] #训练集
test <- d[-train.l,] #测试集
# rpart控制参数设置
ct <- rpart.control(xval=10, minsplit=100, minbucket=40, maxdepth=4) #采用10折交叉验证；停止分裂条件：1、父节点样本小于100；2、叶节点样本大于40；3、深度小于4层；
# rpart建模
fit<-rpart(class~length+exon+rat+FPKM,train,method='class',control=ct,parms = list(split = "information"))
# 可视化
plot(fit,margin=0.1)
text(fit,use.n=T) #原始树
# 剪枝
plotcp(fit)  #筛选合适的cp值
plot(prune(fit,cp=0.01),margin=0.1) 
text(prune(fit,cp=0.01),use.n=T) #length过拟合，增大cp值，减少
plot(prune(fit,cp=0.045),margin=0.1)
text(prune(fit,cp=0.045),use.n=T) 
# 确定模型
fit.used<-prune(fit,cp=0.045)
# 精度
summary(test$type == predict(fit.used,test,type='class'))  #统计预测结果与实际结果是否相同
summary(test$type[test$type != predict(fit.used,test,type='class')]) #统计预测结果与实际一致
summary(test$type[test$type == predict(fit.used,test,type='class')]) #统计预测结果与实际不一致
```
![原始树](/upload/1/raw_tree.png '原始树')
![第一次剪枝](/upload/1/pruned_tree1.png '0.01cp值剪枝结果，过拟合严重。（默认cp值为0.01，所以与原始树一致）')
![第二次剪枝](/upload/1/pruned_tree2.png '最终模型，0.045cp值剪枝结果，主要依靠exon和length属性进行分类')
## 模型分析
最终的模型，只用到了两个属性。这个和之前数据描述相符（长度和外显子数勉强可以区分，表达比例和FPKM重叠严重）。将模型用于测试数据集，模型表现：
![2x2列联表](/upload/1/2x2ContingencyTable.png '2x2列联表')
敏感度$TPR$:
$$TPR=\frac{7546}{7546+332}=0.9578573$$
精度$PPV$:
$$PPV=\frac{7546}{179+7546}=0.9768285$$
特异性$SPC$:
$$SPC=\frac{430}{430+332}=0.5643045$$
准确度$ACC$:
$$ACC=\frac{430+7546}{430+179+332+7546}=0.9397903$$
从上述四个指标上看，除了特异性差很多，其他指标都表现良好。考虑到unknown中有潜在的基因，特异性差也是在预料当中。
在将该模型用于原始数据中unknown部分再进行重新分类，2105个unknown数据中预测出641个潜在的基因，再进一步对模型分类后的数据进行描述：
![预测后数据描述](/upload/1/summary1.png '预测后数据描述')
可以看出，预测后基因与技术噪音在长度和外显子数这两个属性区别更加明显。
# 结论
相对之前的经验判断，使用了CART算法构建技术噪音过滤决策树，避免了主观判断，科学量化了过滤条件。
# 参考文献
1. Duda R O, Hart P E, Stork D G. Pattern classification[M]. John Wiley & Sons, 2012.
2. Prensner J R, Iyer M K, Balbin O A, et al. Transcriptome sequencing across a prostate cancer cohort identifies PCAT-1, an unannotated lincRNA implicated in disease progression[J]. Nature biotechnology, 2011, 29(8): 742-749.
3. Harrington P. 机器学习实战[J]. 人民邮电出版社, 北京, 2013.
